{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Original VGG Paper**:\n",
    "   - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "2. **VGG16 PyTorch Documentation**:\n",
    "   - [torchvision.models.vgg16](https://pytorch.org/docs/stable/generated/torchvision.models.vgg16.html)\n",
    "\n",
    "3. **VGG16 Wikipedia Page**:\n",
    "   - [VGG16 Wikipedia](https://en.wikipedia.org/wiki/VGG16)\n",
    "\n",
    "4. **VGG16 TensorFlow Hub Documentation**:\n",
    "   - [TensorFlow Hub - VGG16 Documentation](https://tfhub.dev/google/imagenet/vgg16/feature_vector/4)\n",
    "\n",
    "5. **VGG16 Implementation in TensorFlow**:\n",
    "   - [VGG16 Implementation in TensorFlow](https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG16 Architecture](https://static.packt-cdn.com/products/9781838827069/graphics/assets/0c28bb91-62aa-4165-a1fe-210d6ab63859.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/ge22m009/miniconda3/lib/python3.12/site-packages (0.16.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (1.44.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ge22m009/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mge23m018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ge22m009/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"PartB.ipynb\"\n",
    "wandb.login()\n",
    "wandb.login(key='13718865007c1068066166d683d0ce0cf87ec304')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the pretrained VGG Net16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:45:12.227090Z",
     "iopub.status.busy": "2023-04-12T02:45:12.226676Z",
     "iopub.status.idle": "2023-04-12T02:45:20.074349Z",
     "shell.execute_reply": "2023-04-12T02:45:20.073432Z",
     "shell.execute_reply.started": "2023-04-12T02:45:12.227055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ge22m009/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/ge22m009/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 246MB/s] \n"
     ]
    }
   ],
   "source": [
    "vggnet = torchvision.models.vgg16(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:45:21.932954Z",
     "iopub.status.busy": "2023-04-12T02:45:21.931877Z",
     "iopub.status.idle": "2023-04-12T02:45:21.941047Z",
     "shell.execute_reply": "2023-04-12T02:45:21.939579Z",
     "shell.execute_reply.started": "2023-04-12T02:45:21.932912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the iNaturalist Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:45:34.665757Z",
     "iopub.status.busy": "2023-04-12T02:45:34.665288Z",
     "iopub.status.idle": "2023-04-12T02:45:39.370358Z",
     "shell.execute_reply": "2023-04-12T02:45:39.368904Z",
     "shell.execute_reply.started": "2023-04-12T02:45:34.665716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 125 3\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_augmentation , train_path, test_path, train_batch_size, val_batch_size, test_batch_size):\n",
    "    # Define the transformation for the input images\n",
    "    transformer1 = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize images to 256x256 pixels\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])  # Normalize images\n",
    "    ])\n",
    "    \n",
    "    # Load the training dataset with the defined transformation\n",
    "    train_Dataset = torchvision.datasets.ImageFolder(train_path, transform=transformer1)\n",
    "    \n",
    "    # Split the training dataset into training and validation sets\n",
    "    train_datasize = int(0.8 * len(train_Dataset))\n",
    "    train_Dataset, val_Dataset = random_split(train_Dataset, [train_datasize, len(train_Dataset) - train_datasize])\n",
    "    \n",
    "    # Apply data augmentation if specified\n",
    "    if data_augmentation == True: \n",
    "        transformer2 = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize images to 256x256 pixels\n",
    "            transforms.RandomHorizontalFlip(0.5),  # Randomly flip images horizontally with a probability of 0.5\n",
    "            transforms.RandomVerticalFlip(0.02),  # Randomly flip images vertically with a probability of 0.02\n",
    "            transforms.RandomRotation(degrees=45),  # Randomly rotate images by up to 45 degrees\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(mean=[0.4602, 0.4495, 0.3800], std=[0.2040, 0.1984, 0.1921])  # Normalize images\n",
    "        ])\n",
    "        \n",
    "        # Create an augmented dataset with the defined transformation\n",
    "        augmented_dataset = torchvision.datasets.ImageFolder(train_path, transform=transformer2)\n",
    "        augmented_dataset_size = int(0.2 * len(augmented_dataset))\n",
    "        \n",
    "        # Split the augmented dataset into training and validation sets\n",
    "        augmented_dataset, _  =  random_split(augmented_dataset, [augmented_dataset_size, len(augmented_dataset) - augmented_dataset_size])\n",
    "        \n",
    "        # Concatenate the original training dataset with the augmented dataset\n",
    "        train_Dataset = ConcatDataset([train_Dataset, augmented_dataset])\n",
    "    \n",
    "    # Create data loaders for the training, validation, and test sets\n",
    "    train_Loader = DataLoader(\n",
    "        train_Dataset, \n",
    "        batch_size=train_batch_size,  # Set the batch size for the training data loader\n",
    "        shuffle=True)  # Shuffle the training data\n",
    "    \n",
    "    test_Loader = DataLoader(\n",
    "        test_path,  # Provide the path to the test dataset\n",
    "        batch_size=test_batch_size,  # Set the batch size for the test data loader\n",
    "        shuffle=True)  # Shuffle the test data\n",
    "    \n",
    "    val_Loader = DataLoader(\n",
    "        val_Dataset, \n",
    "        batch_size=val_batch_size,  # Set the batch size for the validation data loader\n",
    "        shuffle=True)  # Shuffle the validation data\n",
    "    \n",
    "    return train_Loader, val_Loader, test_Loader\n",
    "\n",
    "# Set the paths to the training and test datasets\n",
    "train_path = '/home/ge22m009/inaturalist_12K/train/'  # Path to the training dataset\n",
    "test_path = '/home/ge22m009/inaturalist_12K/val/'  # Path to the test dataset\n",
    "\n",
    "# Define batch sizes for training, validation, and test data loaders\n",
    "train_batch_size = 64\n",
    "test_batch_size = 16\n",
    "val_batch_size = 16\n",
    "\n",
    "# Specify whether to apply data augmentation\n",
    "is_Data_Augmentation = True\n",
    "\n",
    "# Load the datasets and create data loaders\n",
    "train_Loader, val_Loader, test_Loader = load_dataset(is_Data_Augmentation, train_path, test_path, train_batch_size, val_batch_size, test_batch_size)\n",
    "\n",
    "# Print the lengths of the training, validation, and test data loaders\n",
    "print(len(train_Loader), len(val_Loader), len(test_Loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting device to 'cuda' if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:45:49.467231Z",
     "iopub.status.busy": "2023-04-12T02:45:49.466828Z",
     "iopub.status.idle": "2023-04-12T02:45:49.474803Z",
     "shell.execute_reply": "2023-04-12T02:45:49.473541Z",
     "shell.execute_reply.started": "2023-04-12T02:45:49.467192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:45:56.712420Z",
     "iopub.status.busy": "2023-04-12T02:45:56.711980Z",
     "iopub.status.idle": "2023-04-12T02:45:56.718100Z",
     "shell.execute_reply": "2023-04-12T02:45:56.716695Z",
     "shell.execute_reply.started": "2023-04-12T02:45:56.712357Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in vggnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding One more layer to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colab Markdown](https://miro.medium.com/v2/resize:fit:827/1*UeAhoKM0kJfCPA03wt5H0A.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:16.191409Z",
     "iopub.status.busy": "2023-04-12T02:46:16.190938Z",
     "iopub.status.idle": "2023-04-12T02:46:16.202464Z",
     "shell.execute_reply": "2023-04-12T02:46:16.201330Z",
     "shell.execute_reply.started": "2023-04-12T02:46:16.191352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "    (7): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggnet.classifier[6] = torch.nn.Linear(in_features = 4096, out_features = 10)\n",
    "vggnet.classifier.add_module(\"7\", torch.nn.LogSoftmax(dim=1))\n",
    "vggnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:28.826605Z",
     "iopub.status.busy": "2023-04-12T02:46:28.825287Z",
     "iopub.status.idle": "2023-04-12T02:46:28.838418Z",
     "shell.execute_reply": "2023-04-12T02:46:28.837294Z",
     "shell.execute_reply.started": "2023-04-12T02:46:28.826537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "    (7): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model which logs Train accuracy, Train loss, Validataion accuracy & Validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:34.952122Z",
     "iopub.status.busy": "2023-04-12T02:46:34.951715Z",
     "iopub.status.idle": "2023-04-12T02:46:34.966128Z",
     "shell.execute_reply": "2023-04-12T02:46:34.964858Z",
     "shell.execute_reply.started": "2023-04-12T02:46:34.952087Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log): \n",
    "    # Define the loss function\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        model.train()  # Set the model to training mode\n",
    "        # Iterate over the training data\n",
    "        for i, (images, labels) in enumerate(train_Loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward propagation\n",
    "            y_pred = model(images)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_function(y_pred, labels)\n",
    "\n",
    "            # Backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, prediction = torch.max(y_pred.data, 1)\n",
    "            train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "    \n",
    "        # Calculate average training accuracy and loss\n",
    "        train_accuracy /= train_count\n",
    "        train_loss /= train_count\n",
    "        print(f\"Epochs : {epoch+1} Train Accuracy : {train_accuracy} Train Loss {train_loss}\")\n",
    "    \n",
    "        test_accuracy = 0\n",
    "        test_loss = 0\n",
    "        # Evaluate on validation data\n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            # Iterate over the validation data\n",
    "            for i, (images, labels) in enumerate(val_Loader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward propagation\n",
    "                y_pred = model(images)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = loss_function(y_pred, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                test_accuracy += int(torch.sum(predicted == labels.data))\n",
    "\n",
    "            # Calculate average validation accuracy and loss\n",
    "            test_accuracy /= test_count\n",
    "            test_loss /= test_count\n",
    "\n",
    "            print(f\"Epochs : {epoch+1} Validation Accuracy : {test_accuracy} Validation Loss {test_loss}\")\n",
    "            # Log metrics if WandB logging is enabled\n",
    "            if is_wandb_log:\n",
    "                wandb.log({\"train_accuracy\": train_accuracy, \"train_loss\": train_loss, \"val_accuracy\": test_accuracy, \"val_error\": test_loss}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the count of Training dataset & testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:37.262583Z",
     "iopub.status.busy": "2023-04-12T02:46:37.261444Z",
     "iopub.status.idle": "2023-04-12T02:46:37.268054Z",
     "shell.execute_reply": "2023-04-12T02:46:37.266897Z",
     "shell.execute_reply.started": "2023-04-12T02:46:37.262535Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_count(train_path, test_path):\n",
    "    train_count = len(glob.glob(train_path+'/**/*.jpg'))\n",
    "    test_count = len(glob.glob(test_path+'/**/*.jpg'))\n",
    "    print(\"Training dataset count : \", train_count)\n",
    "    print(\"Validation dataset count\", test_count)\n",
    "    return train_count, test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:39.355877Z",
     "iopub.status.busy": "2023-04-12T02:46:39.355463Z",
     "iopub.status.idle": "2023-04-12T02:46:40.010250Z",
     "shell.execute_reply": "2023-04-12T02:46:40.009250Z",
     "shell.execute_reply.started": "2023-04-12T02:46:39.355838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset count :  9999\n",
      "Validation dataset count 2000\n"
     ]
    }
   ],
   "source": [
    "train_count, test_count = get_train_test_count(train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T02:46:50.361393Z",
     "iopub.status.busy": "2023-04-12T02:46:50.360948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ge22m009/Abdul/wandb/run-20240409_231411-26ooew6i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ge23m018/dl2_ge23m018/runs/26ooew6i' target=\"_blank\">trim-frost-1</a></strong> to <a href='https://wandb.ai/ge23m018/dl2_ge23m018' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ge23m018/dl2_ge23m018' target=\"_blank\">https://wandb.ai/ge23m018/dl2_ge23m018</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ge23m018/dl2_ge23m018/runs/26ooew6i' target=\"_blank\">https://wandb.ai/ge23m018/dl2_ge23m018/runs/26ooew6i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 1 Train Accuracy : 0.6791679167916792 Train Loss 0.01493758851974675\n",
      "Epochs : 1 Validation Accuracy : 0.7625 Validation Loss 0.043095723293721674\n",
      "Epochs : 2 Train Accuracy : 0.7414741474147415 Train Loss 0.012013182802574195\n",
      "Epochs : 2 Validation Accuracy : 0.778 Validation Loss 0.04071123447269201\n",
      "Epochs : 3 Train Accuracy : 0.7476747674767477 Train Loss 0.011450672017918764\n",
      "Epochs : 3 Validation Accuracy : 0.777 Validation Loss 0.04020792128145695\n",
      "Epochs : 4 Train Accuracy : 0.7606760676067607 Train Loss 0.011006195582870436\n",
      "Epochs : 4 Validation Accuracy : 0.786 Validation Loss 0.03981682596355677\n",
      "Epochs : 5 Train Accuracy : 0.7641764176417641 Train Loss 0.010849340619033712\n",
      "Epochs : 5 Validation Accuracy : 0.7855 Validation Loss 0.03944969605654478\n",
      "Epochs : 6 Train Accuracy : 0.7724772477247724 Train Loss 0.010720708216055714\n",
      "Epochs : 6 Validation Accuracy : 0.786 Validation Loss 0.03995777470618486\n",
      "Epochs : 7 Train Accuracy : 0.7772777277727773 Train Loss 0.010118173383357394\n",
      "Epochs : 7 Validation Accuracy : 0.785 Validation Loss 0.04017297875881195\n",
      "Epochs : 8 Train Accuracy : 0.7788778877887789 Train Loss 0.01023789468032382\n",
      "Epochs : 8 Validation Accuracy : 0.791 Validation Loss 0.03986186487600207\n"
     ]
    }
   ],
   "source": [
    "# Import the wandb library for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "epochs = 8\n",
    "\n",
    "# Specify whether to log metrics with WandB\n",
    "is_wandb_log = True\n",
    "\n",
    "# Initialize a WandB run with the specified project\n",
    "run = wandb.init(project='dl2_ge23m018')\n",
    "\n",
    "# Train the model using the specified hyperparameters and data loaders\n",
    "train(vggnet, learning_rate, epochs, train_Loader, val_Loader, train_count, test_count, is_wandb_log)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
