# -*- coding: utf-8 -*-
"""DL Assignment02 PART-A-.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QJ55yKONcHfoQ4GBWMQaRmvJymgNdAb
"""

!pip install wandb

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torchvision.models as models
import numpy as np
import wandb

# Prepare dataset
def prepare_dataset(data_dir="/content/drive/MyDrive/DLA02 Dataset", augment_data=False):
    train_dir = os.path.join(data_dir, "train")
    test_dir = os.path.join(data_dir, "val")

    if augment_data:
        train_transforms = transforms.Compose([
            transforms.RandomRotation(90),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomResizedCrop(200),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:
        train_transforms = transforms.Compose([
            transforms.Resize(200),
            transforms.CenterCrop(200),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    test_transforms = transforms.Compose([
        transforms.Resize(200),
        transforms.CenterCrop(200),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
    test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)

    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

    return train_loader, test_loader

# # Custom CNN model
class CustomCNN(nn.Module):
    def __init__(self, num_filters=32, filter_multiplier=1, dropout=0.2, batch_norm=False, dense_size=64, num_classes=10):
        super(CustomCNN, self).__init__()
        self.num_filters = num_filters
        self.filter_multiplier = filter_multiplier
        self.dropout = dropout
        self.batch_norm = batch_norm
        self.dense_size = dense_size

        layers = []
        for i in range(5):
            filter_dim = 11 - 2*i
            if i == 0:
                layers.append(nn.Conv2d(3, num_filters, filter_dim))
            else:
                layers.append(nn.Conv2d(num_filters, num_filters, filter_dim))
            if batch_norm:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            layers.append(nn.MaxPool2d(2))
            num_filters = int(num_filters * filter_multiplier)

        self.features = nn.Sequential(*layers)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(num_filters * 25 * 25, dense_size)
        self.dropout_layer = nn.Dropout(dropout)
        self.fc2 = nn.Linear(dense_size, num_classes)

    def forward(self, x):
        x = self.features(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.dropout_layer(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)
        x = nn.Softmax(dim=1)(x)
        return x
#####

#####
# Train function
def train():
    config_defaults = {
        "num_filters": 32,
        "filter_multiplier": 2,
        "augment_data": False,
        "dropout": 0.3,
        "batch_norm": False,
        "epochs": 10,
        "dense_size": 64,
        "lr": 0.001
    }

    wandb.init(config=config_defaults)
    config = wandb.config

    train_loader, test_loader = prepare_dataset(augment_data=config.augment_data)

    model = CustomCNN(num_filters=config.num_filters, filter_multiplier=config.filter_multiplier,
                      dropout=config.dropout, batch_norm=config.batch_norm, dense_size=config.dense_size)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config.lr)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for epoch in range(config.epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_loss = running_loss / len(train_loader)
        train_accuracy = 100 * correct / total

        wandb.log({"Train Loss": train_loss, "Train Accuracy": train_accuracy})

    # Evaluation
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            test_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_loss /= len(test_loader)
    test_accuracy = 100 * correct / total

    wandb.log({"Val Loss": test_loss, "Val Accuracy": test_accuracy})

# Run sweep
sweep_config = {
    "name": "Test and Save Best Model Again",
    "description": "Checking the performance of CNN on sample of test data",
    "metric": "Val Accuracy",
    "method": "grid",
    "project": "CS6910_Assignment2",
    "parameters": {
        "num_filters": {
            "values": [32]
        },
        "filter_multiplier": {
            "values": [2]
        },
        "augment_data": {
            "values": [True]
        },
        "dropout": {
            "values": [0.3]
        },
        "batch_norm": {
            "values": [True]
        },
        "epochs": {
            "values": [10]
        },
        "dense_size": {
            "values": [64]
        },
        "lr": {
            "values": [0.001]
        }
    }
}

sweep_id = wandb.sweep(sweep_config, project="CS6910_Assignment2")
wandb.agent(sweep_id, function=train)